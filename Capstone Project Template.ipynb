{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration data pipeline \n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is a Udacity Data Engineering Capstone project to showcase all the learning & skills that been acquired during the course of the nano-degree program.<br> This is an open-ended project and for this udacity has provided four datasets that includes US immigration 2016 data, airport codes, temperature and US demographic data.<br> If required, we can add more datasets to enrich information to suit our usecase which we like to analyze or present.<br> \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType, StringType\n",
    "import os, re\n",
    "import configparser\n",
    "import boto3\n",
    "import glob \n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of this project is to build a data Pipeline that utilitize the new technologies and concepts, the project first dump data to s3 bucket, then from that s3 bucket airflow transform and load in AWS Redshift, In this project I am working with the ELTL Extract, Load, Transfer, Load were there are two destinations first destination will be usful for backingup and the end users will be data Scientists and machine learning engineers that data exploration is a part of thier work and they need a large size of data with variation. the second destination is for the data warehouse that will be used for reporting.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"ArchitectureETL.png\"> \n",
    "\n",
    "#### Describe and Gather Data \n",
    "The data used in this project is all given by Udacity I used the Immigration files and extract 2 tables from it, and the demographic dataset, and airport dataset and extract a table from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "    config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\").\\\n",
    "    enableHiveSupport().getOrCreate()\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading the Immigration data from many files to a single data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files appedned is = 11\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Read in the data here\n",
    "i94_file_paths = glob.glob(\"../../data/18-83510-I94-Data-2016/*.sas7bdat\")\n",
    "\n",
    "df_immigration=spark.read.format('com.github.saurfang.sas.spark').\\\n",
    "load(i94_file_paths[0],inferLong=True, forceLowercaseNames=True)\n",
    "            \n",
    "i94_file_paths.pop(0)\n",
    "x = 0\n",
    "for file in i94_file_paths:\n",
    "    tempDF = spark.read.format('com.github.saurfang.sas.spark').\\\n",
    "        load(file,inferLong=True, forceLowercaseNames=True)\n",
    "    if(len(tempDF.columns)==34):\n",
    "        tempDF = tempDF.drop('delete_days','delete_mexl','delete_dup','delete_visa','delete_recdup','dtadfile')\n",
    "    df_immigration.union(tempDF)\n",
    "    x = x+1\n",
    "        \n",
    "print(\"number of files appedned is = \"+ str(x))\n",
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading the airport data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airport = spark.read.csv('inputs/airport-codes_csv.csv')\n",
    "df_airport.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Reading the demographics data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demographics = spark.read.csv('inputs/us-cities-demographics.csv',sep=';')\n",
    "df_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify:\n",
    "1. data quality issues.\n",
    "2. like missing values. \n",
    "3. duplicate data.\n",
    "\n",
    "#### Cleaning Steps\n",
    "1. Change columns name if needed.\n",
    "2. Chnage data types if needed.\n",
    "3. Drop missing data if needed.\n",
    "4. Drop duplicates if it exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### starting this step with the Immigration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Immigration data frame comtains 3096313 record\n",
      "The Immigration data frame comtains 3096313 record\n"
     ]
    }
   ],
   "source": [
    "print('The Immigration data frame comtains '+str(df_immigration.count())+' record')\n",
    "df_immigration1 = df_immigration.dropDuplicates()\n",
    "print('The Immigration data frame comtains '+str(df_immigration1.count())+' record')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### As we can see here there aren't any duplicates in the Immigration data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration1 = df_immigration.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df_immigration.columns])\n",
    "df_immigration1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### We can see that there are missing values some are fine but attributes like entdepa and occup insnum more than 50% of the a records are missing so I might drop them.<br> Aother columns are fine due to there are ZERO duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Same steps for the airport data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The airport data frame comtains 55075 record\n",
      "The airport data frame comtains 55075 record\n",
      "+---+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "| ID|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+---+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|  0|   0|   0|        7006|        0|          0|         0|        5676|   14045|    45886|     26389|          0|\n",
      "+---+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the columns\n",
    "df_airport = df_airport.withColumnRenamed(\"_c0\",\"ID\")\\\n",
    "                            .withColumnRenamed(\"_c1\",\"type\")\\\n",
    "                            .withColumnRenamed(\"_c2\",\"name\")\\\n",
    "                            .withColumnRenamed(\"_c3\",\"elevation_ft\")\\\n",
    "                            .withColumnRenamed(\"_c4\",\"continent\")\\\n",
    "                            .withColumnRenamed(\"_c5\",\"iso_country\")\\\n",
    "                            .withColumnRenamed(\"_c6\",\"iso_region\")\\\n",
    "                            .withColumnRenamed(\"_c7\",\"municipality\")\\\n",
    "                            .withColumnRenamed(\"_c8\",\"gps_code\")\\\n",
    "                            .withColumnRenamed(\"_c9\",\"iata_code\")\\\n",
    "                            .withColumnRenamed(\"_c10\",\"local_code\")\\\n",
    "                            .withColumnRenamed(\"_c11\",\"coordinates\")\n",
    "\n",
    "# removing the first row because it's the header\n",
    "    \n",
    "df_airport = df_airport.where(df_airport.ID != 'ident')\n",
    "print('The airport data frame comtains '+str(df_airport.count())+' record')\n",
    "df_airport1 = df_airport.dropDuplicates()\n",
    "print('The airport data frame comtains '+str(df_airport1.count())+' record')\n",
    "\n",
    "df_airport1 = df_airport.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df_airport.columns])\n",
    "df_airport1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### There are no duplicates in the airport data frame.<br> But there are many missing value in elevation_ft, municipality, gps_code,iata_code local_code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Do the same steps for the demographics data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The demographics data frame comtains 2891 record\n",
      "The demographics data frame comtains 2891 record\n",
      "+----+-----+----------+---------------+-----------------+----------------+-------------------+------------+----------------------+----------+----+-----+\n",
      "|city|state|Median_age|male_population|female_population|total_population|Number_of_ventreans|foreign_born|Average_Household_Size|state_code|race|Count|\n",
      "+----+-----+----------+---------------+-----------------+----------------+-------------------+------------+----------------------+----------+----+-----+\n",
      "|   0|    0|         0|              3|                3|               0|                 13|          13|                    16|         0|   0|    0|\n",
      "+----+-----+----------+---------------+-----------------+----------------+-------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the columns\n",
    "df_demographics = df_demographics.withColumnRenamed(\"_c0\",\"city\")\\\n",
    "                            .withColumnRenamed(\"_c1\",\"state\")\\\n",
    "                            .withColumnRenamed(\"_c2\",\"Median_age\")\\\n",
    "                            .withColumnRenamed(\"_c3\",\"male_population\")\\\n",
    "                            .withColumnRenamed(\"_c4\",\"female_population\")\\\n",
    "                            .withColumnRenamed(\"_c5\",\"total_population\")\\\n",
    "                            .withColumnRenamed(\"_c6\",\"Number_of_ventreans\")\\\n",
    "                            .withColumnRenamed(\"_c7\",\"foreign_born\")\\\n",
    "                            .withColumnRenamed(\"_c8\",\"Average_Household_Size\")\\\n",
    "                            .withColumnRenamed(\"_c9\",\"state_code\")\\\n",
    "                            .withColumnRenamed(\"_c10\",\"race\")\\\n",
    "                            .withColumnRenamed(\"_c11\",\"Count\")\n",
    "    \n",
    "# changing the data types to the appropriate data types.\n",
    "    \n",
    "df_demographics = df_demographics.withColumn(\"city\",col(\"city\").cast(StringType()))\\\n",
    "                            .withColumn(\"state\",col(\"state\").cast(StringType()))\\\n",
    "                            .withColumn(\"Median_age\",col(\"Median_age\").cast(DoubleType()))\\\n",
    "                            .withColumn(\"male_population\",col(\"male_population\").cast(IntegerType()))\\\n",
    "                            .withColumn(\"female_population\",col(\"female_population\").cast(IntegerType()))\\\n",
    "                            .withColumn(\"total_population\",col(\"total_population\").cast(IntegerType()))\\\n",
    "                            .withColumn(\"Number_of_ventreans\",col(\"Number_of_ventreans\").cast(IntegerType()))\\\n",
    "                            .withColumn(\"foreign_born\",col(\"foreign_born\").cast(IntegerType()))\\\n",
    "                            .withColumn(\"Average_Household_Size\",col(\"Average_Household_Size\").cast(DoubleType()))\\\n",
    "                            .withColumn(\"state_code\",col(\"state_code\").cast(StringType()))\\\n",
    "                            .withColumn(\"race\",col(\"race\").cast(StringType()))\\\n",
    "                            .withColumn(\"Count\",col(\"Count\").cast(IntegerType()))\n",
    "# removing the first row because it's the header\n",
    "df_demographics = df_demographics.where(df_demographics.city != 'City')\n",
    "\n",
    "print('The demographics data frame comtains '+str(df_demographics.count())+' record')\n",
    "df_demographics1 = df_demographics.dropDuplicates()\n",
    "print('The demographics data frame comtains '+str(df_demographics1.count())+' record')\n",
    "\n",
    "df_demographics1 = df_demographics.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in df_demographics.columns])\n",
    "df_demographics1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### As we can see there are not duplicates to drop and for the missing values no actions are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "This project is built with a star schema A single Fact table with 3 Dimensions tables\n",
    "* Fact table:\n",
    "    * immigrant_fact\n",
    "    \n",
    "* Dimensions tables:\n",
    "    * airport_dim\n",
    "    * person_dim \n",
    "    * demographic \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"Screen Shot 1444-03-20 at 3.31.24 PM (2).png\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "After cleaning the data (changing types, removing duplicates, dealing with the missing values, changing the columns names)<br>\n",
    "I will drop the columns that will not be used, I will do that by selecting the columns into 4 tables.<br>\n",
    "The below cade is the code used to convert data to the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Fact table Immigration table\n",
    "immigration_table = df_immigration.select('cicid','admnum','i94cit','i94port','arrdate', 'depdate').dropDuplicates(subset=['admnum'])\n",
    "# The person Dimension table will be extracted from df_immigration also\n",
    "person_table = df_immigration.select('admnum','i94res','i94port','i94mode',\\\n",
    "                                        'i94cit','i94mon','arrdate','i94addr',\\\n",
    "                                        'depdate','i94bir','i94visa','occup',\\\n",
    "                                        'biryear','gender','airline',\\\n",
    "                                        'fltno').dropDuplicates(subset=['admnum'])\n",
    "\n",
    "# The airport Dimension will be extracted from df_airport\n",
    "\n",
    "airport_table = df_airport.selectExpr('ID','type','name','iso_country','municipality','iata_code','local_code')\\\n",
    "    .dropDuplicates(subset=['ID'])\n",
    "\n",
    "# The demographic Dimension will be extracted from df_demographics\n",
    "demographics_table = df_demographics.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model.<br>\n",
    "The whole pipeline can be divided into two stages. The first, where we used spark to load, extracted, transform and store the provided datasets into the AWS S3 staging area. The second stage we take advantage of Apache Airflow to build a DAG to extract data from S3 and load them into tables of the same name in Amazon Redshift. As a final step we check the data counting checking to ensure completeness.\n",
    "\n",
    "#### the code for the data pipeline is in the \"airflow/dags/udac_example_dag.py\" file.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"Screen Shot 1444-03-20 at 11.06.27 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "First, we load the Immigrante_table fact table through the step Load_Immigrante_fact_table , which is in parallel run with the steps to load the dimension tables PERSON, AIRPORT ,DEMOGRAPHIC.<br>\n",
    "respectively \n",
    "load_person_dimension_table , load_airport_dimension_table , load_demographic_dimension_table steps. <br>\n",
    "All the tables have a PK constraint that uniquely identify the records and in the fact table there are FK that guarantee that values in the fact are present in the dimension tables.<br>\n",
    "After completing the loading process, we perform a data quality check through the step Data_Quality_Checks to make sure everything was OK. In this check we verify if every table was actually loaded with count check in all the tables of the model.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"Screen Shot 1444-03-20 at 11.06.35 AM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "* #### Tools & Technologies\n",
    "    * Python\n",
    "    * Spark\n",
    "    * AWS S3\n",
    "    * AWS Redshift\n",
    "    * Airflow\n",
    "<br><br>\n",
    "\n",
    "*  Propose how often the data should be updated and why.\n",
    "    * ETL script should be run monthly basis (assuming that new I94 data is available once per month).\n",
    "<br><br>\n",
    "\n",
    "* ### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    " *  The data was increased by 100x?.\n",
    "    * Use Spark to process the data efficiently in a distributed way e.g. with EMR. In case we recognize that we need a write-heavy operation.<br>\n",
    "    * There is no issues with storge when increasing data because of AWS S3, even with pricing it's not expensive.\n",
    "    <br><br>\n",
    "\n",
    " *  The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    * Dags can be scheduled to run daily by setting the start_date config as a datetime value containing both the date and time when it should start running, then setting schedule_interval to @daily which will ensure the dag runs everyday at the time provided in start_date.\n",
    " <br><br>\n",
    "\n",
    " *  The database needed to be accessed by 100+ people.\n",
    "    * Redshift can support upto 500 connections, so 100+ people can easily connect to Redshift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Project-Capstone provides tools to automatically process, clean, analyze US I94 Immigration data in a flexible way and help answering questions like <br> <br>\n",
    "   * Which airline people used most to immigrate US ?.<br>\n",
    "   * Which US airports people used most to immigrate US?.<br>\n",
    "   * What immigrante gender is more ?."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
